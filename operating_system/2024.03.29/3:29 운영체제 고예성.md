# OS 3/29

- **단기, 중기, 장기 스케쥴러에 대해 설명해 주세요.**
    - `장기 스케줄러`는 HDD 상의 프로그램을 커널에 등록(or 레디 큐 등록)할 때의 스케줄러입니다.
    - `단기 스케줄러`는 레디 큐의 프로세스를 CPU에 할당하여 실행 상태로 만들 때의 스케줄러입니다.
    - `중기 스케줄러`는 메모리에 적재된 프로세스 수를 관리하는 스케줄러입니다.
    
    - **현대 OS에는 단기, 중기, 장기 스케쥴러를 모두 사용하고 있나요?**
        
        현대 OS에서는 대부분 장기 스케줄러를 두지 않습니다. 
        
        훨씬 적은 양의 메모리를 활용했던 과거에는 프로세스에 효율적으로 메모리를 할당하기 위해 장기 스케줄러의 역할이 필요했지만, 비교적 많은 양의 메모리를 활용하는 현대에는 프로세스가 시작되면 장기 스케줄러 없이 바로 프로세스에 메모리를 할당하여 장기 스케쥴러의 작업 없이 레디 큐에 넣어주게 됩니다.
        
    - **프로세스의 스케줄링 상태에 대해 설명해 주세**
        1. **Running(실행 상태):**
            - Running 상태는 CPU를 사용하여 프로세스가 실행되는 상태를 말한다.
            - 이 상태에서는 해당 프로세스가 CPU를 독점하며, 해당 프로세스가 실행되는 작업만을 수행
        2. **Ready(준비 상태):**
            - Ready 상태는 CPU가 할당되기 위해 대기하고 있는 상태
            - CPU 스케쥴링 알고리즘에 따라 다음에 CPU를 할당받음
        3. **Blocked(차단 상태):**
            - Blocked 상태는 어떤 이벤트가 발생하여 프로세스가 실행을 일시 중지한 상태
            - 이벤트는 입출력(I/O) 작업, 시그널, 자원 요청 등이 있음
            - 해당 이벤트가 해결될 때까지 대기하다가 이벤트가 해결되면 Ready 상태로 돌아간다.
        4. **Suspended(중지 상태):**
            - Suspended 상태는 프로세스가 메모리에서 제거되어 디스크에 저장된 상태
            - 프로세스가 메모리에서 제거되었으므로, CPU가 할당되지 않는다.
        5. **Terminated(종료 상태):**
            - Terminated 상태는 프로세스가 실행을 완료하거나, 운영 체제에 의해 강제로 종료된 상태
            - 더 이상 CPU를 사용하지 않으며, 자원들이 반환된다.
            
    - **preemptive/non-preemptive 에서 존재할 수 없는 상태가 있을까요?**
        
        **Preemptive(선점형) 스케줄링은**
        
        운영체제가 CPU를 현재 실행 중인 프로세스에서 *강제로 빼앗아* 다른 프로세스에 할당하는 방식이다. 즉, 운영체제는 프로세스의 우선순위나 실행 시간 등을 고려하여 현재 실행 중인 프로세스보다 우선순위가 높은 프로세스가 대기 중이면 해당 프로세스로 CPU를 할당한다.
        
        따라서, Preemptive 스케줄링에서는 프로세스가 실행 중에도 언제든지 중지될 수 있기 때문에 응답 시간을 줄일 수 있고, *시스템 전체적인 성능을 향상*시킬 수 있습니다.
        
        **Non-preemptive(비선점형) 스케줄링은**
        
        현재 실행 중인 프로세스가 CPU를 *자발적으로 반납할 때까지* 다른 프로세스에 CPU를 할당하지 않는 방식이다. 이 경우, 운영체제는 *프로세스가 자발적으로 CPU를 반납할 때까지 대기*하게 된다.
        
        따라서, Non-preemptive 스케줄링에서는 프로세스가 실행 중인 동안에는 언제든지 중지될 수 없기 때문에 우선순위가 높은 프로세스가 대기 중에 있더라도 실행 중인 프로세스가 모두 실행될 때까지 기다려야 한다. 이로 인해 *응답 시간이 느리고, 시스템 성능이 저하*될 가능성이 있다.
        
        Preemptive는 현재 실행 중인 프로세스를 강제로 중단시켜 다른 프로세스에게 CPU를 할당할 수 있지만, Non-preemptive는 현재 실행 중인 프로세스가 자발적으로 CPU를 반납하기 전까지 다른 프로세스가 CPU를 할당받을 수 없다.
        
        Preemptive와 Non-preemptive는 CPU 스케쥴링 알고리즘에서 사용되는 개념이므로, 프로세스의 실행 상태와는 별도로 존재한다. 따라서 Preemptive와 Non-preemptive에서 존재할 수 없는 상태는 따로 존재하지 않는다.
        
        따라서, Preemptive와 Non-preemptive에서 프로세스는 실행, 준비, 차단, 중지, 종료 등의 상태를 가질 수 있으며, 이러한 상태 변화는 CPU 스케쥴링 알고리즘에 의해 결정됩니다.
        
    - **Memory가 부족할 경우, Process는 어떠한 상태로 변화할까요?**
        1. **Swap 파일 사용**: 운영체제는 메모리 부족 상황에서 하드 디스크의 일부를 메모리처럼 사용할 수 있는 Swap 파일을 사용한다. Swap 파일은 물리적 메모리보다 느리기 때문에 성능 저하가 발생할 수 있지만, 시스템이 다운되지 않도록 하는 역할을 하게 된다.
        2. **프로세스 종료**: 운영체제는 사용하지 않는 프로세스를 종료하여 메모리를 확보할 수 있다. 일반적으로 우선순위가 낮거나 사용자에 의해 종료될 수 있는 프로세스가 종료된다.
        3. **메모리 압축**: 운영체제는 메모리를 압축하여 빈 공간을 확보할 수 있다. 이 방법은 메모리에서 데이터를 이동하고 변경하는 데 많은 시간이 걸리므로 성능이 저하될 수 있습니다.
        
- **컨텍스트 스위칭 시에는 어떤 일들이 일어나나요?**
    1. PCB에 현재 컨텍스트 저장 및 커널 모드로 전환
    2. 다음 실행할 프로세스 결정
    3. PC, 레지스터 등 PCB 에서 로드
    4. 사용자 모드로 전환
    
    - **프로세스와 스레드는 컨텍스트 스위칭이 발생했을 때 어떤 차이가 있을까요?**
        
        프로세스 단위에 컨텍스트 스위칭은 스레드보다 비용이 많이드는 작업입니다. 스레드는 메모리공간을 공유하기 때문에 PC, Stack Pointer 등 일부 값만 변경되기 때문입니다.
        
    - **컨텍스트 스위칭이 발생할 때, 기존의 프로세스 정보는 커널스택에 어떠한 형식으로 저장되나요?**
        
        PCB에 PC, register, 메모리 관리 정보, 스택 및 힙 포인터, 기타 프로세스 정보 등의 정보를 포함하여 저장합니다.
        
    - **컨텍스트 스위칭은 언제 일어날까요?**
        
        단기 스케줄러에 의해 CPU 작업이 바뀌거나 시스템 자원의 응답을 기다리는 대기상태 일때 일어나게 됩니다. 혹은 사용자가 임의로 지정한 스레드 스케줄링에 의해 발생합니다.
        

- **프로세스 스케줄링 알고리즘에는 어떤 것들이 있나요?**
    
    **First Come, First Served (FCFS)**: 먼저 도착한 프로세스가 먼저 CPU를 할당받는 가장 간단한 스케줄링 알고리즘입니다.
    
    - 장점
        - 구현이 간단하고 이해하기 쉽습니다.
    - 단점
        - 평균 응답 시간이 길 수 있습니다. 실행 시간이 긴 프로세스가 먼저 도착하면 뒤에 있는 짧은 작업들이 오랜 시간 기다려야 합니다. 이로 인해 대기 시간이 증가하고 응답 시간이 느려집니다.
        - Convoy Effect: 긴 프로세스가 먼저 도착하여 CPU를 점유하고 있는 동안, 뒤에 있는 작업들이 해당 프로세스 주위에 모이는 현상이 발생할 수 있습니다. 이로 인해 짧은 프로세스들이 대기 시간이 길어지는 Convoy Effect가 발생할 수 있습니다.
    
    ---
    
    **Shortest Job First (SJF)**: 실행 시간이 가장 짧은 프로세스를 우선적으로 처리하는 스케줄링 알고리즘입니다. Non-preemptive와 Preemptive 두 가지 형태가 있습니다.
    
    - 장점
        - 평균 대기 시간을 최소화합니다. 실행 시간이 짧은 프로세스가 먼저 처리되므로 평균 대기 시간이 줄어듭니다.
    - 단점
        - 실행 시간을 사전에 알아야 하며, 실행 시간을 예측하기 어려울 수 있습니다.
        - Starvation: 실행 시간이 긴 프로세스가 짧은 프로세스들에게 계속해서 밀려날 수 있어, 실행 시간이 긴 프로세스가 Starvation될 수 있습니다.
    
    ---
    
    **STCF(Shortest Time-to-Completion First)**는 프로세스 스케줄링 알고리즘 중 하나입니다. 이 알고리즘은 현재 실행 중인 프로세스와 대기 중인 프로세스 중에서 가장 짧은 남은 실행 시간을 가진 프로세스에게 CPU를 할당합니다.
    
    STCF는 Shortest Remaining Time First(SRTF)라고도 알려지며, SJF(Shortest Job First)의 선점형 버전으로 볼 수 있습니다. STCF는 다음과 같은 특징을 가집니다:
    
    1. **선점형 스케줄링**: 현재 실행 중인 프로세스의 남은 실행 시간보다 더 짧은 실행 시간을 가진 새로운 프로세스가 도착하면, 현재 실행 중인 프로세스를 중단하고 새로운 프로세스에 CPU를 할당합니다.
    2. **최소 실행 시간 우선**: 가장 짧은 남은 실행 시간을 가진 프로세스가 우선적으로 CPU를 할당받습니다.
    
    STCF의 장점은 다음과 같습니다:
    
    - 평균 대기 시간을 최소화합니다.
    - 응답 시간을 최소화합니다.
    - 짧은 프로세스들에게 우선적으로 CPU를 할당하여 전체적인 시스템 성능을 향상시킵니다.
    
    그러나 STCF는 다음과 같은 단점도 가집니다:
    
    - 실행 시간을 사전에 알아야 합니다. 이는 실행 시간을 예측하기 어려운 경우에 문제가 될 수 있습니다.
    - Starvation: 실행 시간이 긴 프로세스가 계속해서 짧은 프로세스들에 의해 밀려나 CPU를 할당받지 못할 수 있습니다.
    
    ---
    
    **Priority Scheduling**: 각 프로세스에 우선순위를 부여하고, 우선순위가 높은 프로세스를 먼저 처리하는 스케줄링 알고리즘입니다. Non-preemptive와 Preemptive 두 가지 형태가 있습니다.
    
    - 장점
        - 우선순위에 따라 중요한 작업을 먼저 처리할 수 있습니다.
    - 단점
        - Starvation: 우선순위가 낮은 프로세스가 계속해서 우선순위가 높은 프로세스에 의해 밀려나 CPU를 할당받지 못할 수 있습니다.
        - 무한 봉쇄(Indefinite Blocking): 우선순위가 높은 프로세스가 무한히 실행되어 우선순위가 낮은 프로세스가 무한히 대기하는 현상이 발생할 수 있습니다.
        
    
    ---
    
    **Round Robin (RR)**: 각 프로세스에 일정한 시간 슬라이스(quantum)를 할당하고, 시간이 지나면 다음 프로세스로 CPU를 넘기는 방식으로 처리하는 스케줄링 알고리즘입니다.
    
    - 장점
        - 모든 프로세스가 일정한 시간 동안 CPU를 할당받기 때문에 공정한 스케줄링을 제공합니다.
        - 응답 시간을 보장합니다.
    - 단점
        - Time Slice의 크기가 너무 작으면 문맥 교환 오버헤드가 증가할 수 있습니다.
        - Time Slice의 크기가 너무 크면 대화형 작업의 응답 시간이 느려질 수 있습니다.
    
    ---
    
    **Multi-level Queue Scheduling**: 프로세스들을 여러 개의 큐에 그룹화하고, 각 큐마다 다른 우선순위를 가지며 스케줄링하는 알고리즘입니다.
    
    **장점**
    
    1. **우선순위에 따른 스케줄링:** 각 큐에는 다른 우선순위를 부여하고, 우선순위가 높은 큐에 있는 프로세스가 먼저 처리됩니다. 이를 통해 시스템 성능을 최적화할 수 있습니다.
    2. **서로 다른 작업 부하 처리:** 각 큐는 다른 종류의 작업 부하를 처리하기 위해 사용될 수 있습니다. 예를 들어, 높은 우선순위 큐는 실시간 작업을 처리하고, 낮은 우선순위 큐는 배치 작업을 처리할 수 있습니다.
    
    **단점**
    
    1. **스타베이션:** 우선순위가 낮은 큐에 있는 프로세스가 항상 우선순위가 높은 큐에 있는 프로세스에 의해 밀려나 CPU를 할당받지 못할 수 있습니다. 이는 스타베이션 문제를 야기할 수 있습니다.
    2. **정확한 우선순위 설정의 어려움:** 적절한 우선순위 설정이 필요하며, 이를 위해 사용자가 많은 조정이 필요할 수 있습니다.
    
    ---
    
    **Multi-level Feedback Queue (MLFQ)**: 다중 큐를 사용하여 프로세스를 스케줄링하는 알고리즘으로, 각 큐마다 다른 우선순위와 다른 Time Slice를 할당합니다.
    
    **장점**
    
    1. **다양한 우선순위 설정:** MLFQ는 여러 개의 큐와 다양한 우선순위 설정을 사용합니다. 이를 통해 다양한 종류의 작업을 효과적으로 처리할 수 있습니다.
    2. **유연한 우선순위 조정:** 프로세스의 동작에 따라 우선순위를 조절할 수 있습니다. 이는 시스템의 요구에 맞게 우선순위를 조정할 수 있다는 장점을 제공합니다.
    
    **단점**
    
    1. **구현 복잡성:** MLFQ는 구현이 복잡할 수 있습니다. 다양한 큐와 우선순위 설정을 관리하기 위해 추가적인 로직이 필요합니다.
    2. **스케줄링 오버헤드:** 큐 간의 이동이나 우선순위 변경 등의 작업은 스케줄링 오버헤드를 초래할 수 있습니다.
    
    - **RR을 사용할 때, Time Slice에 따른 trade-off를 설명해 주세요.**
        
        Time Slice가 길어진다면 응답 시간이 나빠지지만 컨텍스트 스위칭 오버헤드가 작아집니다. 
        
        반면 Time Slice 가 짧아진다면 응답시간이 좋아지지만 컨텍스트 스위칭 오버헤드가 커집니다.
        
        Round Robin(RR) 스케줄링 알고리즘에서 Time Slice(시간 슬라이스)는 각 프로세스가 CPU를 할당받는 시간을 결정하는 매개변수입니다. Time Slice의 크기에 따라 다음과 같은 trade-off가 발생할 수 있습니다:
        
        1. **Time Slice가 짧은 경우**:
            - 작은 Time Slice를 사용하면 CPU가 빠르게 다른 프로세스로 전환됩니다.
            - 짧은 Time Slice는 프로세스 간 응답 시간을 향상시킬 수 있습니다. 모든 프로세스가 빠르게 CPU를 할당받아 작업을 수행할 수 있습니다.
            - 하지만, 너무 작은 Time Slice는 문맥 교환(Context Switching) 오버헤드를 증가시킬 수 있습니다. 이는 시스템 성능에 부담을 줄 수 있습니다.
            
        2. **Time Slice가 긴 경우**:
            - 큰 Time Slice를 사용하면 각 프로세스가 CPU를 오랫동안 소유합니다. 이는 문맥 교환 오버헤드를 줄일 수 있습니다.
            - 또한, 큰 Time Slice는 CPU를 점유하는 프로세스의 처리량을 증가시킬 수 있습니다. 각 프로세스는 긴 시간 동안 CPU를 사용할 수 있기 때문입니다.
            - 그러나, 너무 큰 Time Slice는 대화형 작업의 응답 시간을 감소시킬 수 있습니다. 짧은 대기 시간을 요구하는 프로세스는 CPU를 할당받을 때까지 기다려야 하므로, 응답 시간이 길어질 수 있습니다.
        
        따라서, Time Slice의 크기를 결정할 때는 프로세스의 특성과 시스템의 요구 사항을 고려해야 합니다. 적절한 Time Slice를 선택하여 응답 시간을 극대화하고 시스템 성능을 최적화하는 것이 중요합니다.
        
    - **싱글 스레드 CPU 에서 상시로 돌아가야 하는 프로세스가 있다면, 어떤 스케쥴링 알고리즘을 사용하는 것이 좋을까요? 또 왜 그럴까요?**
        
        싱글 스레드 CPU에서 상시로 돌아가야 하는 프로세스가 있다면, 가장 적합한 스케줄링 알고리즘은 짧은 타임 Slice로 동시에 실행하는것처럼 느낄 수 있는 **Round Robin**일 것입니다.
        
        **Round Robin** 알고리즘은 각 프로세스가 일정한 시간 슬라이스(quantum)를 할당받고, 이 시간이 지나면 다음 프로세스에게 CPU를 넘깁니다. 이 알고리즘은 다음과 같은 이유로 싱글 스레드 CPU에서 상시로 돌아가야 하는 프로세스에게 적합합니다:
        
        1. **공정한 CPU 할당**: Round Robin은 각 프로세스에게 CPU 시간을 공평하게 분배합니다. 따라서 상시로 돌아가야 하는 프로세스가 다른 프로세스와 공정하게 CPU를 공유할 수 있습니다.
        2. **응답 시간 보장**: Round Robin은 각 프로세스에게 일정한 시간 슬라이스를 할당하므로, 모든 프로세스가 일정한 시간 내에 CPU를 할당받을 수 있습니다. 따라서 응답 시간이 보장되며, 사용자나 시스템의 대기 시간이 최소화됩니다.
        3. **간단한 구현**: Round Robin은 간단한 스케줄링 알고리즘이며, 구현이 간단합니다. 따라서 싱글 스레드 CPU에서 상시로 돌아가야 하는 프로세스에게 쉽게 적용할 수 있습니다.
        4. **실시간성**: Round Robin은 실시간성이 보장되는 알고리즘 중 하나입니다. 프로세스들이 일정한 시간 슬라이스를 할당받기 때문에, 실시간 요구에 응답하는 데 적합합니다.
        
        따라서 싱글 스레드 CPU에서 상시로 돌아가야 하는 프로세스에게는 Round Robin 알고리즘이 적합합니다.
        
    - **동시성과 병렬성의 차이에 대해 설명해 주세요.**
        
        동시성은 한 장치에서 여러 작업을 동시에 수행하는 기능을 말하는 것으로 멀티태스킹, 멀티스레딩과 같은 방식으로 이루어집니다. 
        
        **동시성(Concurrency)**:
        
        - 동시성은 여러 작업이 동시에 실행되는 것처럼 보이는 개념입니다. 하나의 시스템에서 동시에 여러 작업이 실행되는 것처럼 보이지만, 실제로는 각 작업이 겹치는 시간 동안 번갈아가며 실행됩니다.
        - 동시성은 작업들이 독립적으로 실행되는 것이 아니라, 서로 다른 작업들이 시분할(Time-sharing) 방식으로 실행되는 것을 의미합니다. 이러한 작업들은 동시에 시작하고 종료될 필요는 없습니다.
        
        반면에 병렬성은 여러 장치를 통해 여러 작업을 처리하는 것으로 멀티 프로세싱, 분산처리 등과 같은 기술로 이루어집니다.
        
        **병렬성(Parallelism)**:
        
        - 병렬성은 여러 작업이 동시에 실행되는 것을 의미합니다. 병렬성은 여러 프로세스 또는 스레드가 실제로 동시에 실행되는 것을 나타냅니다.
        - 병렬성은 하나의 시스템에서 여러 개의 처리 장치(CPU 코어, GPU 등)를 사용하여 작업을 분할하고 병렬로 실행함으로써 달성됩니다. 이는 작업들이 동시에 시작하고 동시에 종료될 수 있음을 의미합니다.
        
        간단히 말하면, 동시성은 여러 작업들이 시간을 나눠가며 실행되는 것처럼 보이는 개념이며, 병렬성은 실제로 여러 작업이 동시에 실행되는 것을 나타냅니다.
        
        예를 들어, 하나의 CPU에서 여러 프로세스가 동시에 실행되는 것처럼 보이지만, 이는 동시성이며, 여러 개의 CPU 코어에서 각각의 프로세스가 동시에 실행되는 것은 병렬성입니다.
        
    - **타 스케쥴러와 비교하여, Multi-level Feedback Queue는 어떤 문제점들을 해결한다고 볼 수 있을까요?**
        
        MLFQ 는 오랫동안 할당되지 않는 프로세스틀의 우선순위를 높이는 boost 기능이 있어 starvation 문제를 해결할 수 있습니다.
        
        Multi-level Feedback Queue(MFQ)는 다중 큐를 사용하여 프로세스를 스케줄링하는 스케줄링 알고리즘입니다. 이 알고리즘은 여러 문제점들을 해결할 수 있습니다. 다른 스케줄러와 비교하여 MFQ가 해결하는 주요 문제점은 다음과 같습니다:
        
        1. **스루풋(Throughput) 및 응답 시간 개선**: MFQ는 여러 개의 큐를 사용하여 각 프로세스를 다양한 우선순위로 구분합니다. 이를 통해 짧은 CPU 버스트를 가진 프로세스가 우선적으로 처리되어 응답 시간을 개선하고, CPU를 효율적으로 활용하여 스루풋을 향상시킵니다.
        2. **스타베이션(Starvation) 방지**: MFQ는 각 큐에 프로세스를 우선순위에 따라 배치하므로 낮은 우선순위의 프로세스가 영원히 대기하지 않고 CPU를 할당받을 수 있습니다. 이를 통해 스타베이션 문제를 방지하고 모든 프로세스가 공평하게 CPU를 이용할 수 있습니다.
        3. **실시간 및 대화형 작업 처리**: MFQ는 다양한 우선순위 수준을 가지고 있으므로 실시간 및 대화형 작업을 효과적으로 처리할 수 있습니다. 높은 우선순위 큐에 실시간 작업을 배치하여 신속하게 처리하고, 낮은 우선순위 큐에는 CPU를 오랜 시간 점유하는 작업을 배치하여 시스템의 반응성을 유지할 수 있습니다.
        4. **다양한 작업 부하 처리**: MFQ는 다양한 유형의 작업 부하를 처리할 수 있습니다. 짧은 CPU 버스트를 가진 대화형 작업과 긴 CPU 버스트를 가진 배치 작업 등 각기 다른 특성을 가지는 작업을 효율적으로 처리할 수 있습니다.
        
        따라서, Multi-level Feedback Queue는 다양한 종류의 작업을 처리하고 시스템의 성능을 향상시키는 데 도움이 되는 많은 문제들을 해결할 수 있습니다.
        
    - **FIFO 스케쥴러는 정말 쓸모가 없는 친구일까요? 어떤 시나리오에 사용하면 좋을까요?**
        
        FIFO(First In, First Out) 스케쥴러는 가장 간단한 스케쥴링 알고리즘 중 하나입니다. 이 알고리즘은 프로세스가 큐에 도착한 순서대로 실행되는 방식으로 동작합니다. FIFO 스케쥴러는 특정 상황에서 유용할 수 있지만, 모든 상황에 적합하지는 않습니다.
        
        FIFO 스케쥴러의 주요 특징은 다음과 같습니다:
        
        1. **간단함**: FIFO 스케쥴링은 구현이 간단하며 이해하기 쉽습니다.
        2. **비선점형(Non-preemptive) 방식**: 프로세스가 CPU를 할당받으면 완료될 때까지 CPU를 계속 사용합니다. 따라서 다른 프로세스가 CPU를 강제로 빼앗을 수 없습니다.
        
        FIFO 스케쥴러는 다음과 같은 상황에서 유용할 수 있습니다:
        
        1. **배치 처리 시스템**: 배치 처리 시스템에서는 프로세스의 상대적인 우선순위가 중요하지 않을 때 FIFO 스케쥴러를 사용할 수 있습니다. 예를 들어, 일괄 처리 작업이 큐에 도착한 순서대로 처리될 때 유용합니다.
        2. **단순한 시스템 요구**: 작업이 간단하고 우선순위가 거의 고려되지 않는 시스템에서는 FIFO 스케쥴러를 사용할 수 있습니다. 예를 들어, 실시간 시스템이 아니며, 프로세스의 실행 순서가 중요하지 않은 경우에 해당합니다.
        
        그러나 FIFO 스케쥴러는 다음과 같은 단점이 있습니다:
        
        1. **스루풋(Throughput) 및 응답 시간**: FIFO 스케쥴링은 평균 응답 시간이나 스루풋 면에서 다른 스케쥴러에 비해 효율적이지 않을 수 있습니다. 특히, CPU 버스트 시간이 긴 프로세스가 먼저 도착하면, 뒤에 있는 짧은 CPU 버스트를 갖는 프로세스들이 오랜 시간 기다려야 할 수 있습니다.
        2. **스타베이션(Starvation)**: 스케쥴링 대기열에 계속해서 새로운 프로세스가 도착할 경우, FIFO 스케쥴러는 먼저 도착한 프로세스가 계속 CPU를 점유하게 되어 후속 프로세스들이 CPU를 할당받지 못하는 스타베이션 문제가 발생할 수 있습니다.
        
        따라서, FIFO 스케쥴러는 특정한 환경에서는 유용할 수 있지만, 모든 상황에 적합하지는 않습니다. 실제로 사용되는 많은 운영 체제에서는 더 복잡한 스케쥴링 알고리즘을 사용합니다.
        
    - **우리는 스케줄링 알고리즘을 "프로세스" 스케줄링 알고리즘이라고 부릅니다. 스레드는 다른 방식으로 스케줄링을 하나요?**
        1. **프로세스 스케줄링**: 프로세스 스케줄링은 운영 체제가 시스템의 다양한 프로세스 간에 CPU를 할당하는 방법을 관리합니다. 각 프로세스는 독립적인 주소 공간을 가지며, 각각의 프로세스는 하나 이상의 스레드를 포함할 수 있습니다. 프로세스 스케줄러는 각 프로세스가 CPU를 할당받는 순서와 우선순위를 결정합니다. 이러한 스케줄링은 주로 프로세스의 컨텍스트 스위칭과 관련되어 있습니다.
        
        1. **스레드 스케줄링**: 스레드 스케줄링은 주어진 프로세스 내에서 여러 스레드 간에 CPU 시간을 분배하는 방법을 관리합니다. 스레드는 동일한 프로세스 내에서 공유되는 리소스를 사용하므로 스레드 간의 스케줄링은 더 세밀한 수준에서 이루어집니다. 이러한 스케줄링은 주로 스레드 간의 경쟁 조건이나 데드락 등의 문제를 해결하기 위해 설계되었습니다.
        
        따라서, 프로세스 스케줄링은 운영 체제가 프로세스 간에 CPU를 할당하는 것을 관리하고, 스레드 스케줄링은 프로세스 내의 스레드 간에 CPU를 할당하는 것을 관리합니다. 둘 다 서로 다른 목표와 요구 사항을 충족시키기 위해 디자인되었습니다.
        
        1. **Round Robin**: 각 스레드는 일정한 시간 슬라이스(quantum)를 할당받고, 이 시간이 지나면 다음 스레드에게 CPU를 넘깁니다. Round Robin 알고리즘은 공정한 CPU 할당을 보장하고, 프로세스 간의 응답 시간을 균등하게 분배하는 데 유용합니다.
        2. **Priority-based Scheduling**: 각 스레드에는 우선순위가 할당되고, 높은 우선순위를 가진 스레드가 CPU를 더 많이 할당받습니다. 이러한 알고리즘은 시스템의 성능을 최적화하기 위해 특정 작업에 대한 우선순위를 지정할 수 있도록 해줍니다.
        3. **Multilevel Queue Scheduling**: 스레드들을 여러 개의 큐에 그룹화하고, 각 큐마다 다른 우선순위를 가집니다. 낮은 우선순위의 큐는 CPU를 자주 점유하지 못하고, 높은 우선순위의 큐는 CPU를 자주 점유하게 됩니다. 이는 다양한 종류의 작업을 분류하고 관리하는 데 유용합니다.
        4. **Fair-share Scheduling**: 시스템 리소스를 여러 사용자 또는 그룹에게 공평하게 분배하는 방식으로 스레드를 스케줄링합니다. 각 사용자 또는 그룹은 자신의 공정한 할당량을 받습니다.
        5. **Earliest Deadline First (EDF)**: 스레드의 마감 시간을 기준으로 가장 빠른 마감 시간을 가진 스레드가 CPU를 할당받는 방식으로 스레드를 스케줄링합니다. 이 알고리즘은 실시간 시스템에서 사용되며, 마감 시간을 보장해야 하는 작업에 유용합니다.
        
    - **유저 스레드와 커널 스레드의 스케쥴링 알고리즘은 똑같을까요?**
        
        유저 스레드와 커널 스레드는 서로 다른 스케줄링 계층에서 작동합니다. 따라서 스레드의 유형에 따라 스케줄링 알고리즘이 다를 수 있습니다.
        
        1. **유저 스레드 (User-level Threads)**: 유저 스레드는 스레드 관리가 사용자 수준에서 이루어지며, 운영 체제는 해당 스레드를 인식하지 않습니다. 유저 수준 스레드의 스케줄링은 일반적으로 사용자 수준 라이브러리나 런타임에 의해 관리됩니다. 이러한 스레드는 유저 공간에서 동작하므로 커널에는 해당 스레드에 대한 정보가 없습니다. 따라서 유저 스레드의 스케줄링은 사용자 레벨에서 구현되며, 일반적으로 사용자가 직접 또는 사용자 수준 스레드 라이브러리에 의해 제공된 스케줄링 알고리즘을 따릅니다.
        
        1. **커널 스레드 (Kernel-level Threads)**: 커널 스레드는 운영 체제 커널에 의해 직접 관리되는 스레드입니다. 따라서 커널은 커널 스레드를 인식하고 스케줄링을 수행합니다. 일반적으로 운영 체제 커널은 프로세스의 컨텍스트 스위칭과 스레드의 생성, 스케줄링 등을 처리하는데, 이 과정에서 커널 스레드의 스케줄링 알고리즘이 적용됩니다.
        
        따라서 유저 스레드와 커널 스레드는 서로 다른 스케줄링 레벨에서 작동하며, 따라서 스케줄링 알고리즘도 다를 수 있습니다. 유저 스레드의 경우 사용자 수준에서 스케줄링되지만, 커널 스레드는 운영 체제 커널의 영향을 받아 스케줄링됩니다.
        

- **동기와 비동기, 블로킹과 논블로킹의 차이에 대해 설명해 주세요.**
    1. **동기 (Synchronous)**:
        - 동기적 작업은 요청된 작업이 완료될 때까지 대기하고, 작업의 결과를 기다립니다. 즉, 작업을 순차적으로 처리하고, 다음 작업은 이전 작업이 완료된 후에 시작됩니다.
        - 동기 작업에서는 호출한 함수나 작업이 반환되기를 기다리며, 작업이 완료될 때까지 현재 스레드나 프로세스는 대기 상태에 있습니다.
    2. **비동기 (Asynchronous)**:
        - 비동기적 작업은 요청된 작업을 시작하고, 완료 여부와 상관없이 다른 작업을 수행할 수 있습니다. 즉, 작업이 완료되기를 기다리지 않고, 작업을 시작하고 다른 작업을 수행할 수 있습니다.
        - 비동기 작업에서는 호출한 함수나 작업이 즉시 반환되며, 작업이 백그라운드에서 비동기적으로 처리됩니다. 작업이 완료되면 콜백 또는 이벤트를 통해 결과를 처리할 수 있습니다.
    3. **블로킹 (Blocking)**:
        - 블로킹 작업은 작업이 완료될 때까지 프로그램이나 스레드가 대기 상태에 있습니다. 이는 일반적으로 동기 작업에 해당합니다. 작업이 완료될 때까지 제어가 반환되지 않습니다.
    4. **논블로킹 (Non-blocking)**:
        - 논블로킹 작업은 작업이 완료될 때까지 대기하지 않고, 작업이 완료되지 않아도 프로그램이나 스레드가 다른 작업을 수행할 수 있습니다. 이는 주로 비동기 작업에 해당합니다. 작업을 시작하고 바로 제어가 반환되며, 작업이 완료될 때까지 다른 작업을 수행할 수 있습니다.
    
    요약하자면, 동기적 작업은 작업 완료를 기다리고, 작업이 완료될 때까지 대기 상태에 있으며, 블로킹될 수 있습니다. 반면에 비동기적 작업은 작업 완료를 기다리지 않고, 작업을 시작하고 다른 작업을 수행할 수 있으며, 블로킹되지 않습니다.
    
    여러분이 카페에 가서 커피를 주문한 상황입니다.
    
    1. **동기적 작업 (Synchronous)**:
        - 동기적으로 작업을 처리하는 것은 주문한 커피가 준비될 때까지 카페에서 대기하는 것과 같습니다. 주문한 커피가 완성될 때까지 다른 일을 할 수 없고, 계속해서 대기 상태에 있어야 합니다.
    2. **비동기적 작업 (Asynchronous)**:
        - 반면에 비동기적으로 작업을 처리하는 것은 주문한 후에 다른 곳으로 이동하여 다른 일을 할 수 있는 것과 같습니다. 주문한 커피가 준비될 때까지 대기하지 않고, 다른 작업을 처리하거나 다른 장소에서 활동할 수 있습니다.
    3. **블로킹 (Blocking)**:
        - 블로킹 작업은 주문한 커피가 준비될 때까지 카페에서 대기하는 것과 같습니다. 주문한 커피가 준비되기 전에는 다른 일을 처리할 수 없고, 계속해서 카페에 머물러야 합니다.
    4. **논블로킹 (Non-blocking)**:
        - 논블로킹 작업은 주문한 후에 카페를 나와서 다른 장소로 이동하여 다른 일을 처리할 수 있는 것과 같습니다. 주문한 커피가 준비될 때까지 대기하지 않고, 다른 장소로 이동하여 자유롭게 활동할 수 있습니다.
    
    - **그렇다면, 동기이면서 논블로킹이고, 비동기이면서 블로킹인 경우는 의미가 있다고 할 수 있나요?**
        
        동기이면서 논블로킹인 경우와 비동기이면서 블로킹인 경우는 특정 상황에 따라 의미가 있을 수 있습니다.
        
        - 동기-블로킹 : 블로킹의 관점은 제어권에 있기 때문에 다른 작업이 시작되는 동안 동작하지 않습니다. 동기의 관점은 결과의 처리이기 때문에 결과를 반환하면 해당 업무를 바로 처리하게 됩니다.
            
            ex) 자바 입출력
            
        - 동기-논블로킹 : 논블로킹은 자신의 제어권을 가지고 일을 하는 것입니다. 그런데 동기는 결과에 관심이 많으므로 중간중간마다 결과값이 나왔는지 주기적으로 물어보다가 결과값이 나오면, 해당 결과를 가지고 업무를 처리합니다.
            
            이와 같은 동기-논블로킹 방식은 특정 작업의 결과가 필요한 경우에도 프로그램의 실행 흐름이 일시 중단되지 않고 다른 작업을 병렬로 처리할 수 있게 해줍니다.
            
        - 비동기-블로킹 : 블로킹이기 때문에 자신의 작업에 대한 제어권이 없고, 비동기이기 때문에 결과를 바로 처리하지 않아도 됩니다. 일반적으로 사용되지 않고, 특수한 상황에서만 사용됩니다.
            
            
        - 비동기-논블로킹 : 논블로킹은 다른 작업이 시작되어도 자신이 하던 작업은 멈추지 않습니다. 따라서 양쪽에서 서로 각자 작업을 처리하게 되고, 다른 작업이 결과가 끝나면 바로 처리하지 않고 자신의 일이 끝나면 처리하게 됩니다.
        
        1. **동기이면서 논블로킹 (Synchronous Non-blocking)**:
            - 이는 작업을 순차적으로 처리하면서도 다른 작업을 블로킹하지 않는 경우를 의미합니다.
            - 예를 들어, 파일을 동기적으로 읽어오되, 파일이 준비될 때까지 대기하지 않고, 대신에 다른 작업을 수행하거나 루프를 통해 주기적으로 확인하는 것입니다. 이러한 방식은 블로킹 작업을 피하면서도 작업을 순차적으로 처리하는 데 도움이 될 수 있습니다.
        2. **비동기이면서 블로킹 (Asynchronous Blocking)**:
            - 이는 비동기적으로 작업을 수행하되, 작업이 완료될 때까지 대기하는 경우를 의미합니다.
            - 예를 들어, 비동기적으로 파일을 읽어오되, 파일이 완전히 읽힐 때까지 대기하는 것입니다. 이러한 방식은 비동기적으로 작업을 수행하면서도 작업이 완료될 때까지 대기하여 결과를 동기적으로 처리하는 데 사용될 수 있습니다.
        
    - **I/O 멀티플렉싱에 대해 설명해 주세요.**
        
        여러 I/O 작업을 동시에 처리하거나 관리하기 위한 기술입니다. 이 기술을 사용하면 프로그램이 하나의 스레드 또는 프로세스에서 동시에 여러 파일 디스크립터(file descriptor) 또는 소켓(socket)을 효율적으로 관리할 수 있습니다. I/O 멀티플렉싱은 주로 네트워크 서버, 데이터베이스 관리 시스템(DBMS), 웹 서버 등과 같이 많은 I/O 작업이 발생하는 프로그램에서 사용됩니다.
        
        I/O 멀티플렉싱의 주요 목적은 프로그램이 블로킹 없이 여러 I/O 작업을 동시에 처리할 수 있도록 하는 것입니다. 즉, 프로그램이 특정 I/O 작업을 기다리는 동안 다른 I/O 작업을 수행할 수 있습니다. 이를 통해 프로그램의 처리량(throughput)을 향상시키고 자원 사용을 최적화할 수 있습니다.
        
        I/O 멀티플렉싱을 이용하는 네트워크 서버를 생각해보겠습니다. 이 서버는 여러 클라이언트로부터 동시에 연결을 수락하고, 각 클라이언트의 요청을 처리해야 합니다. I/O 멀티플렉싱을 사용하면, 서버는 하나의 스레드 또는 프로세스에서 동시에 여러 클라이언트의 요청을 처리할 수 있으며, 블로킹 없이 효율적으로 작업을 수행할 수 있습니다.
        
        만약, I/O 멀티플렉싱을 사용하지 않는다면 서버는 다중 스레드/프로세스 또는 순차적 처리 방식을 사용하여 클라이언트 요청을 처리해야 합니다. 이러한 방식들은 각각 시스템 자원 사용과 처리량에 대한 문제를 갖고 있으며, I/O 멀티플렉싱을 사용하는 방식보다 효율적이지 않을 수 있습니다.
        
        I/O 멀티플렉싱(I/O Multiplexing)은 단일 스레드 또는 프로세스가 여러 개의 I/O 작업을 동시에 관리할 수 있도록 하는 기술입니다. 이를 통해 한 스레드나 프로세스가 여러 개의 I/O 작업을 감시하고, 작업 중 하나가 완료될 때까지 대기하거나 다른 작업을 수행할 수 있습니다.
        
        I/O 멀티플렉싱은 주로 네트워크 프로그래밍에서 사용되며, 다음과 같은 장점을 제공합니다:
        
        1. **단일 스레드/프로세스로 다중 I/O 관리**: 단일 스레드 또는 프로세스가 여러 개의 소켓이나 파일 디스크립터를 관리할 수 있습니다. 이는 멀티스레드나 멀티프로세스 방식에 비해 더 효율적인 자원 사용을 가능하게 합니다.
        2. **비동기적 작업 처리**: 멀티플렉싱을 통해 여러 개의 I/O 작업을 비동기적으로 처리할 수 있습니다. 한 작업이 완료될 때까지 기다리는 대신, 작업이 완료되는 즉시 이벤트를 처리하고 다른 작업을 수행할 수 있습니다.
        3. **간단한 구현**: 멀티플렉싱을 구현하는 것은 비교적 간단합니다. 대부분의 운영 체제는 멀티플렉싱을 위한 시스템 콜을 제공하며, 이를 활용하여 간단하게 구현할 수 있습니다.
        
        ```java
        import java.io.IOException;
        import java.net.InetSocketAddress;
        import java.nio.ByteBuffer;
        import java.nio.channels.SelectionKey;
        import java.nio.channels.Selector;
        import java.nio.channels.ServerSocketChannel;
        import java.nio.channels.SocketChannel;
        import java.util.Iterator;
        import java.util.Set;
        
        public class MultiplexingEchoServer {
            private static final int PORT = 8888;
        
            public static void main(String[] args) throws IOException {
                // Selector 생성
                Selector selector = Selector.open();
        
                // 서버 소켓 채널 생성
                ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();
                serverSocketChannel.bind(new InetSocketAddress(PORT));
                serverSocketChannel.configureBlocking(false); // 논블로킹 모드로 설정
                serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); // 연결 수락 이벤트 등록
        
                System.out.println("Server started on port " + PORT);
        
                // 이벤트 루프 시작
                while (true) {
                    selector.select(); // 이벤트 발생을 대기
        
                    // 발생한 이벤트들을 처리
                    Set<SelectionKey> selectedKeys = selector.selectedKeys();
                    Iterator<SelectionKey> iterator = selectedKeys.iterator();
                    while (iterator.hasNext()) {
                        SelectionKey key = iterator.next();
                        iterator.remove();
        
                        if (key.isAcceptable()) { // 연결 수락 이벤트 처리
                            acceptConnection(key);
                        } else if (key.isReadable()) { // 읽기 이벤트 처리
                            readFromClient(key);
                        }
                    }
                }
            }
        
            private static void acceptConnection(SelectionKey key) throws IOException {
                ServerSocketChannel serverSocketChannel = (ServerSocketChannel) key.channel();
                SocketChannel clientChannel = serverSocketChannel.accept();
                clientChannel.configureBlocking(false);
                clientChannel.register(key.selector(), SelectionKey.OP_READ); // 읽기 이벤트 등록
                System.out.println("Accepted connection from " + clientChannel.getRemoteAddress());
            }
        
            private static void readFromClient(SelectionKey key) throws IOException {
                SocketChannel clientChannel = (SocketChannel) key.channel();
                ByteBuffer buffer = ByteBuffer.allocate(1024);
                int bytesRead = clientChannel.read(buffer);
                if (bytesRead == -1) { // 클라이언트가 연결을 닫은 경우
                    clientChannel.close();
                    return;
                }
                buffer.flip();
                byte[] data = new byte[bytesRead];
                buffer.get(data);
                System.out.println("Received from client: " + new String(data));
                // 받은 데이터를 다시 클라이언트로 보냄 (에코)
                clientChannel.write(ByteBuffer.wrap(data));
            }
        }
        
        ```
        
    - **논블로킹 I/O를 수행한다고 하면, 그 결과를 어떻게 수신할 수 있나요?**
        
        논블로킹 I/O를 수행한 후 결과를 수신하는 방법은 일반적으로 다음과 같은 절차를 따릅니다:
        
        1. **작업 시작**: 먼저 논블로킹 모드로 I/O 작업을 시작합니다. 이를 통해 작업이 완료될 때까지 대기하지 않고 다른 작업을 수행할 수 있습니다.
        2. **작업 상태 확인**: I/O 작업이 완료되었는지 여부를 주기적으로 확인합니다. 이를 위해 보통 비동기적 이벤트를 사용하거나, 반복적으로 상태를 확인하는 방식을 사용합니다.
        3. **완료된 결과 수신**: I/O 작업이 완료되었다는 이벤트를 감지하면 결과를 수신합니다. 이를 위해 보통 콜백 함수나 이벤트 핸들러를 등록하여 완료된 결과를 처리합니다.
        4. **결과 처리**: 결과를 수신하여 필요한 작업을 수행합니다. 이는 결과를 분석하고 적절한 작업을 수행하는 단계입니다.
        
        논블로킹 I/O를 수행한 후 결과를 수신하는 방법은 Java의 NIO(Non-blocking I/O) 패키지를 사용하여 구현할 수 있습니다. 다음은 클라이언트가 서버로부터 데이터를 읽는 간단한 예제 코드입니다.
        
        ```
        
        ```
        
        ```java
        import java.io.IOException;
        import java.net.InetSocketAddress;
        import java.nio.ByteBuffer;
        import java.nio.channels.SocketChannel;
        
        public class NonBlockingClient {
            private static final String SERVER_HOST = "localhost";
            private static final int SERVER_PORT = 8888;
        
            public static void main(String[] args) {
                try {
                    // 소켓 채널 생성 및 연결
                    SocketChannel socketChannel = SocketChannel.open();
                    socketChannel.configureBlocking(false); // 논블로킹 모드 설정
                    socketChannel.connect(new InetSocketAddress(SERVER_HOST, SERVER_PORT));
        
                    // 연결 완료 여부 확인
                    while (!socketChannel.finishConnect()) {
                        // 연결 완료되기 전까지 다른 작업 수행 가능
                        System.out.println("Waiting for connection...");
                    }
        
                    // 데이터 수신을 위해 버퍼 생성
                    ByteBuffer buffer = ByteBuffer.allocate(1024);
        
                    // 서버로부터 데이터 읽기
                    int bytesRead = socketChannel.read(buffer);
                    if (bytesRead != -1) {
                        buffer.flip(); // 버퍼를 읽기 위한 상태로 전환
                        byte[] data = new byte[bytesRead];
                        buffer.get(data); // 읽은 데이터를 배열로 복사
                        System.out.println("Received from server: " + new String(data));
                    } else {
                        System.out.println("Connection closed by server");
                    }
        
                    // 소켓 채널 닫기
                    socketChannel.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
        ```